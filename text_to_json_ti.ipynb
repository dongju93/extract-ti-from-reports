{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# win 2012 path\n",
    "directory_path = 'C:/Users/spdlq/Documents/my_code/ttp_autogpt/python_ground/text/2015-2022_text/'\n",
    "output_path = 'C:/Users/spdlq/Documents/my_code/ttp_autogpt/python_ground/output/2015-2022/'\n",
    "\n",
    "# mac 2012-2022 path\n",
    "# directory_path = '/Users/dong-ju/Documents/My_code/ttp_autogpt/python_ground/text/2012-2022_text/'\n",
    "# output_path = '/Users/dong-ju/Documents/My_code/ttp_autogpt/python_ground/output/2012-2022/'\n",
    "\n",
    "# mac 2015-2022 path\n",
    "# directory_path = '/Users/dong-ju/Documents/My_code/ttp_autogpt/python_ground/text/2015-2022_ocr/'\n",
    "# output_path = '/Users/dong-ju/Documents/My_code/ttp_autogpt/python_ground/output/2015-2022_ocr/'\n",
    "\n",
    "# winHome 2015-2022 path\n",
    "# directory_path = 'C:/Users/DongJu/Documents/my_code/python_ground/text/2015-2022_text/'\n",
    "# output_path = 'C:/Users/DongJu/Documents/my_code/python_ground/output/2015-2022/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references(content):\n",
    "    reference_name_pattern = re.compile(r'CVE-\\d{4}-\\d{4,5}', re.IGNORECASE)\n",
    "    return [match.upper() for match in re.findall(reference_name_pattern, content)]\n",
    "\n",
    "# Custom rules for non-English filename\n",
    "def extract_filenames(content):\n",
    "    filename_pattern = re.compile(r'\\b\\w+\\.[a-zA-Z]{2,4}\\b')\n",
    "    potential_filenames = re.findall(filename_pattern, content)\n",
    "    cleaned_filenames = []\n",
    "    for filename in potential_filenames:\n",
    "        name, ext = filename.rsplit('.', 1)\n",
    "        english_part = re.search('[a-zA-Z]+$', name)\n",
    "        if english_part:\n",
    "            name = english_part.group()\n",
    "        cleaned_filenames.append(f\"{name}.{ext}\")\n",
    "    return cleaned_filenames\n",
    "\n",
    "def extract_md5(content): \n",
    "    md5_pattern = re.compile(r'\\b[0-9a-fA-F]{32}\\b')\n",
    "    return re.findall(md5_pattern, content)\n",
    "\n",
    "def extract_sha1(content): \n",
    "    sha1_pattern = re.compile(r'\\b[0-9a-fA-F]{40}\\b')\n",
    "    return re.findall(sha1_pattern, content)\n",
    "\n",
    "def extract_sha256(content): \n",
    "    sha256_pattern = re.compile(r'\\b[0-9a-fA-F]{64}\\b')\n",
    "    return re.findall(sha256_pattern, content)\n",
    "\n",
    "def extract_registry_entries(content):\n",
    "    registry_pattern = re.compile(r'HK[A-Z_]+\\\\[^\"\\n]+')\n",
    "    return re.findall(registry_pattern, content)\n",
    "\n",
    "def extract_urls(content):\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return re.findall(url_pattern, content)\n",
    "\n",
    "# weight is Dummy\n",
    "def categorize_content(content, rule_id, filename):\n",
    "    # Extracting data from content\n",
    "    references = extract_references(content)\n",
    "    samples = extract_filenames(content)\n",
    "\n",
    "    return {\n",
    "        'rule_id': rule_id,\n",
    "        'name': filename[:-4],\n",
    "        'description': \"-\",\n",
    "        'references': extract_references(content),\n",
    "        'samples': extract_filenames(content),\n",
    "        'md5' : extract_md5(content),\n",
    "        'sha1' : extract_sha1(content),\n",
    "        'sha256' : extract_sha256(content),\n",
    "        'Registry_Entries': extract_registry_entries(content),\n",
    "        'URLs': extract_urls(content),\n",
    "        'weight': 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text files\n",
    "def get_text_files_from_directory(directory_path):\n",
    "    return [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.txt')]\n",
    "\n",
    "# Save json files\n",
    "def write_to_json(output_path, content):\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        json.dump(content, outfile)\n",
    "        # json.dump(content, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White lists\n",
    "\n",
    "whitelist_urls = [\n",
    "    \"www.trendmicro.com\",\n",
    "    \"attack.mitre.org\",\n",
    "    \"documents.trendmicro.com\",\n",
    "    \"www.ithome.com.tw\",\n",
    "    \"hitcon.org\"\n",
    "]\n",
    "\n",
    "whitelist_file_names = [\n",
    "    \".com\",\n",
    "    \".org\",\n",
    "    \".www\",\n",
    "    \".jp\",\n",
    "    \".net\",\n",
    "    \".cn\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique content for each files\n",
    "def process_text_file(file_path, unique_sets, rule_id):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        filename = os.path.basename(file_path)\n",
    "        categorized_content = categorize_content(content, rule_id, filename)\n",
    "\n",
    "        categorized_content['references'] = list(set([x for x in categorized_content['references'] if x not in unique_sets['references']]))\n",
    "        unique_sets['references'].update(categorized_content['references'])\n",
    "\n",
    "        categorized_content['samples'] = list(set([x for x in categorized_content['samples'] if x not in unique_sets['samples'] and not any(whitelisted in x for whitelisted in whitelist_file_names)]))\n",
    "        unique_sets['samples'].update(categorized_content['samples'])\n",
    "\n",
    "        categorized_content['Registry_Entries'] = list(set([x for x in categorized_content['Registry_Entries'] if x not in unique_sets['Registry_Entries']]))\n",
    "        unique_sets['Registry_Entries'].update(categorized_content['Registry_Entries'])\n",
    "\n",
    "        categorized_content['md5'] = list(set([x for x in categorized_content['md5'] if x not in unique_sets['md5']]))\n",
    "        unique_sets['md5'].update(categorized_content['md5'])\n",
    "\n",
    "        categorized_content['sha1'] = list(set([x for x in categorized_content['sha1'] if x not in unique_sets['sha1']]))\n",
    "        unique_sets['sha1'].update(categorized_content['sha1'])\n",
    "\n",
    "        categorized_content['sha256'] = list(set([x for x in categorized_content['sha256'] if x not in unique_sets['sha256']]))\n",
    "        unique_sets['sha256'].update(categorized_content['sha256'])\n",
    "\n",
    "        categorized_content['URLs'] = list(set([x for x in categorized_content['URLs'] if x not in unique_sets['URLs'] and not any(whitelisted in x for whitelisted in whitelist_urls)]))\n",
    "        unique_sets['URLs'].update(categorized_content['URLs'])\n",
    "\n",
    "        keys_to_combine = ['md5', 'sha1', 'sha256', 'Registry_Entries']\n",
    "        categorized_content['signatures'] = [item for key in keys_to_combine for item in categorized_content.get(key, [])]\n",
    "        \n",
    "        return categorized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule_id is auto incresement\n",
    "def process_all_text_files(directory_path, output_path):\n",
    "    text_files = get_text_files_from_directory(directory_path)\n",
    "    unique_sets = {\n",
    "        'references': set(),\n",
    "        'samples': set(),\n",
    "        'Registry_Entries': set(),\n",
    "        'md5': set(),\n",
    "        'sha1': set(),\n",
    "        'sha256': set(),\n",
    "        'URLs': set()\n",
    "    }\n",
    "    rule_id = 0\n",
    "    for file_path in text_files:\n",
    "        rule_id += 1\n",
    "        categorized_content = process_text_file(file_path, unique_sets, rule_id)\n",
    "        # Remove individual md5, sha1, sha256, and Registry_Entries fields before writing to JSON\n",
    "        for field in ['md5', 'sha1', 'sha256', 'Registry_Entries', 'URLs']:\n",
    "            categorized_content.pop(field, None)\n",
    "        filename_without_extension = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_json_path = os.path.join(output_path, filename_without_extension + \".json\")\n",
    "        write_to_json(output_json_path, categorized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excute\n",
    "process_all_text_files(directory_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
