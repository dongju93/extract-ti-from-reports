{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from envs.env import text_path, json_path, ntfy_nofi\n",
    "from envs.whitelist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = text_path\n",
    "output_path = json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_using_pattern(pattern, content):\n",
    "    return re.findall(pattern, content)\n",
    "\n",
    "\n",
    "def extract_references(content):\n",
    "    reference_name_patterns = [r\"CVE-\\d{4}-\\d{4,5}\", r\"T\\d{3,5}\"]\n",
    "    references = []\n",
    "    for pattern in reference_name_patterns:\n",
    "        matches = extract_using_pattern(pattern, content)\n",
    "        for match in matches:\n",
    "            references.append(match.upper())\n",
    "    return references\n",
    "\n",
    "\n",
    "# def extract_filenames(content):\n",
    "#     filename_pattern = r\"\\b\\w+\\.[a-zA-Z]{2,4}\\b\"\n",
    "#     potential_filenames = extract_using_pattern(filename_pattern, content)\n",
    "\n",
    "#     cleaned_filenames = []\n",
    "#     for filename in potential_filenames:\n",
    "#         name, ext = filename.rsplit(\".\", 1)\n",
    "#         english_part = re.search(\"[a-zA-Z]+$\", name)\n",
    "#         if english_part:\n",
    "#             name = english_part.group()\n",
    "#         cleaned_filenames.append(f\"{name}.{ext}\")\n",
    "#     return cleaned_filenames\n",
    "\n",
    "\n",
    "def extract_md5(content):\n",
    "    md5_pattern = r\"\\b[0-9a-fA-F]{32}\\b\"\n",
    "    return extract_using_pattern(md5_pattern, content)\n",
    "\n",
    "\n",
    "def extract_sha1(content):\n",
    "    sha1_pattern = r\"\\b[0-9a-fA-F]{40}\\b\"\n",
    "    return extract_using_pattern(sha1_pattern, content)\n",
    "\n",
    "\n",
    "def extract_sha256(content):\n",
    "    sha256_pattern = r\"\\b[0-9a-fA-F]{64}\\b\"\n",
    "    return extract_using_pattern(sha256_pattern, content)\n",
    "\n",
    "\n",
    "# def extract_registry_entries(content):\n",
    "#     registry_pattern = r'HK[A-Z_]+\\\\[^\"\\n]+'\n",
    "#     return extract_using_pattern(registry_pattern, content)\n",
    "\n",
    "\n",
    "def extract_URL(content):\n",
    "    url_pattern = (\n",
    "        # r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|\"\n",
    "        # r\"(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "        r\"\\b[\\w\\[\\]\\.]*\\[\\.\\][\\w\\[\\]\\.]*\\b\"\n",
    "    )\n",
    "    URL = extract_using_pattern(url_pattern, content)\n",
    "    # Remove \"[\" and \"]\" from the extracted URL\n",
    "    cleaned_URL = [url.replace(\"[\", \"\").replace(\"]\", \"\") for url in URL]\n",
    "    return cleaned_URL\n",
    "\n",
    "\n",
    "def categorize_content(content, rule_id, filename):\n",
    "    return {\n",
    "        \"rule_id\": rule_id,\n",
    "        \"name\": filename[:-4],\n",
    "        # \"description\": \"-\",\n",
    "        \"references\": extract_references(content),\n",
    "        # \"samples\": extract_filenames(content),\n",
    "        \"MD5\": extract_md5(content),\n",
    "        \"SHA1\": extract_sha1(content),\n",
    "        \"SHA256\": extract_sha256(content),\n",
    "        # \"Registry_Entries\": extract_registry_entries(content),\n",
    "        \"URL\": extract_URL(content),\n",
    "        \"weight\": 0.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text files\n",
    "def get_text_files_from_directory(directory_path):\n",
    "    return [\n",
    "        os.path.join(directory_path, file)\n",
    "        for file in os.listdir(directory_path)\n",
    "        if file.endswith(\".txt\")\n",
    "    ]\n",
    "\n",
    "\n",
    "# Save json files\n",
    "def write_to_json(output_path, content):\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        # json.dump(content, outfile)\n",
    "        json.dump(content, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unique_content(category, content, unique_sets, whitelist=None):\n",
    "    filtered_content = list(set([x for x in content if x not in unique_sets[category]]))\n",
    "    if whitelist:\n",
    "        filtered_content = [\n",
    "            x\n",
    "            for x in filtered_content\n",
    "            if not any(whitelisted in x for whitelisted in whitelist)\n",
    "        ]\n",
    "    unique_sets[category].update(filtered_content)\n",
    "    return filtered_content\n",
    "\n",
    "\n",
    "def combine_into_signatures(categorized_content):\n",
    "    keys_to_combine = [\"MD5\", \"SHA1\", \"SHA256\", \"URL\"]\n",
    "    return [\n",
    "        item for key in keys_to_combine for item in categorized_content.get(key, [])\n",
    "    ]\n",
    "\n",
    "\n",
    "def process_text_file(file_path, unique_sets, rule_id):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        content = file.read()\n",
    "        filename = os.path.basename(file_path)\n",
    "        categorized_content = categorize_content(content, rule_id, filename)\n",
    "\n",
    "        categorized_content[\"references\"] = filter_unique_content(\n",
    "            \"references\", categorized_content[\"references\"], unique_sets\n",
    "        )\n",
    "        # categorized_content[\"samples\"] = filter_unique_content(\n",
    "        #     \"samples\", categorized_content[\"samples\"], unique_sets, whitelist_file_names\n",
    "        # )\n",
    "        # categorized_content[\"Registry_Entries\"] = filter_unique_content(\n",
    "        #     \"Registry_Entries\", categorized_content[\"Registry_Entries\"], unique_sets\n",
    "        # )\n",
    "        categorized_content[\"MD5\"] = filter_unique_content(\n",
    "            \"MD5\", categorized_content[\"MD5\"], unique_sets\n",
    "        )\n",
    "        categorized_content[\"SHA1\"] = filter_unique_content(\n",
    "            \"SHA1\", categorized_content[\"SHA1\"], unique_sets\n",
    "        )\n",
    "        categorized_content[\"SHA256\"] = filter_unique_content(\n",
    "            \"SHA256\", categorized_content[\"SHA256\"], unique_sets\n",
    "        )\n",
    "        categorized_content[\"URL\"] = filter_unique_content(\n",
    "            \"URL\", categorized_content[\"URL\"], unique_sets, whitelist_urls\n",
    "        )\n",
    "\n",
    "        # merge into signatures category\n",
    "        categorized_content[\"signatures\"] = combine_into_signatures(categorized_content)\n",
    "\n",
    "        return categorized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_text_files(directory_path, output_path):\n",
    "    text_files = get_text_files_from_directory(directory_path)\n",
    "\n",
    "    unique_sets = {\n",
    "        \"references\": set(),\n",
    "        # \"samples\": set(),\n",
    "        # \"Registry_Entries\": set(),\n",
    "        \"MD5\": set(),\n",
    "        \"SHA1\": set(),\n",
    "        \"SHA256\": set(),\n",
    "        \"URL\": set(),\n",
    "    }\n",
    "\n",
    "    rule_id = 1\n",
    "    for file_path in text_files:\n",
    "        categorized_content = process_text_file(file_path, unique_sets, rule_id)\n",
    "\n",
    "        # Toggle remove fields\n",
    "        fields_to_remove = [\"MD5\", \"SHA1\", \"SHA256\", \"URL\"]\n",
    "        for field in fields_to_remove:\n",
    "            categorized_content.pop(field, None)\n",
    "\n",
    "        # don't create files if signatures is empoy\n",
    "        if not categorized_content.get(\"signatures\"):\n",
    "            print(\"No signatures found for:\", file_path)\n",
    "            continue\n",
    "        rule_id += 1\n",
    "        filename_without_extension = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_json_path = os.path.join(\n",
    "            output_path, filename_without_extension + \".json\"\n",
    "        )\n",
    "        write_to_json(output_json_path, categorized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excute\n",
    "process_all_text_files(directory_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
