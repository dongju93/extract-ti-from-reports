{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_content(content):\n",
    "    reference_name_pattern = re.compile(r'\\bCVC[^\\s]*\\b')\n",
    "    reference_name = re.findall(reference_name_pattern, content)\n",
    "    filename_pattern = re.compile(r'\\b\\w+\\.\\w+\\b')\n",
    "    filenames = re.findall(filename_pattern, content)\n",
    "    md5_pattern = re.compile(r'\\b[0-9a-fA-F]{32}\\b')\n",
    "    sha1_pattern = re.compile(r'\\b[0-9a-fA-F]{40}\\b')\n",
    "    sha256_pattern = re.compile(r'\\b[0-9a-fA-F]{64}\\b')\n",
    "    md5_hashes = re.findall(md5_pattern, content)\n",
    "    sha1_hashes = re.findall(sha1_pattern, content)\n",
    "    sha256_hashes = re.findall(sha256_pattern, content)\n",
    "    registry_pattern = re.compile(r'HK[A-Z_]+\\\\[^\"]+')\n",
    "    registry_entries = re.findall(registry_pattern, content)\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    urls = re.findall(url_pattern, content)\n",
    "\n",
    "    return {\n",
    "        'Reference_Name': reference_name,\n",
    "        'File_Names': filenames,\n",
    "        'MD5_Hashes': md5_hashes,\n",
    "        'SHA1_Hashes': sha1_hashes,\n",
    "        'SHA256_Hashes': sha256_hashes,\n",
    "        'Registry_Entries': registry_entries,\n",
    "        'URLs': urls\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets to track unique items across all files\n",
    "unique_reference_names = set()\n",
    "unique_filenames = set()\n",
    "unique_md5_hashes = set()\n",
    "unique_sha1_hashes = set()\n",
    "unique_sha256_hashes = set()\n",
    "unique_registry_entries = set()\n",
    "unique_urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'YOUR_DIRECTORY_PATH' with the path to the directory where your .txt files are located\n",
    "directory_path = '/Users/dong-ju/Documents/My_code/ttp_autogpt/files'\n",
    "\n",
    "# Generate a list of file paths for all the .txt files in that directory\n",
    "text_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.txt')]\n",
    "\n",
    "\n",
    "# Dictionary to store categorized content for all files\n",
    "all_files_categorized_content = {}\n",
    "\n",
    "for file_path in text_files:\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        categorized_content = categorize_content(content)\n",
    "        filename = os.path.basename(file_path)\n",
    "        formatted_filename = 'TTP_files: ' + filename[:-4]\n",
    "        \n",
    "        # Check for and remove duplicates across all files\n",
    "        categorized_content['Reference_Name'] = [x for x in categorized_content['Reference_Name'] if x not in unique_reference_names]\n",
    "        unique_reference_names.update(categorized_content['Reference_Name'])\n",
    "        \n",
    "        categorized_content['File_Names'] = [x for x in categorized_content['File_Names'] if x not in unique_filenames]\n",
    "        unique_filenames.update(categorized_content['File_Names'])\n",
    "        \n",
    "        categorized_content['MD5_Hashes'] = [x for x in categorized_content['MD5_Hashes'] if x not in unique_md5_hashes]\n",
    "        unique_md5_hashes.update(categorized_content['MD5_Hashes'])\n",
    "        \n",
    "        categorized_content['SHA1_Hashes'] = [x for x in categorized_content['SHA1_Hashes'] if x not in unique_sha1_hashes]\n",
    "        unique_sha1_hashes.update(categorized_content['SHA1_Hashes'])\n",
    "        \n",
    "        categorized_content['SHA256_Hashes'] = [x for x in categorized_content['SHA256_Hashes'] if x not in unique_sha256_hashes]\n",
    "        unique_sha256_hashes.update(categorized_content['SHA256_Hashes'])\n",
    "        \n",
    "        categorized_content['Registry_Entries'] = [x for x in categorized_content['Registry_Entries'] if x not in unique_registry_entries]\n",
    "        unique_registry_entries.update(categorized_content['Registry_Entries'])\n",
    "        \n",
    "        categorized_content['URLs'] = [x for x in categorized_content['URLs'] if x not in unique_urls]\n",
    "        unique_urls.update(categorized_content['URLs'])\n",
    "        \n",
    "        # Add the cleaned data to the output dictionary\n",
    "        all_files_categorized_content[formatted_filename] = categorized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire dictionary as a single JSON file\n",
    "output_json_path = \"/Users/dong-ju/Documents/My_code/ttp_autogpt/files/output/output.json\"\n",
    "with open(output_json_path, \"w\") as outfile:\n",
    "    json.dump(all_files_categorized_content, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
