{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'C:/Users/spdlq/Documents/my_code/ttp_autogpt/python_ground/2022_text'\n",
    "output_path = 'C:/Users/spdlq/Documents/my_code/ttp_autogpt/python_ground/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references(content):\n",
    "    reference_name_pattern = re.compile(r'CVE-\\d{4}-\\d{4,5}', re.IGNORECASE)\n",
    "    return [match.upper() for match in re.findall(reference_name_pattern, content)]\n",
    "\n",
    "def extract_filenames(content):\n",
    "    filename_pattern = re.compile(r'\\b\\w+\\.[a-zA-Z]{2,4}\\b')\n",
    "    return re.findall(filename_pattern, content)\n",
    "\n",
    "def extract_hashes(content):\n",
    "    md5_pattern = re.compile(r'\\b[0-9a-fA-F]{32}\\b')\n",
    "    sha1_pattern = re.compile(r'\\b[0-9a-fA-F]{40}\\b')\n",
    "    sha256_pattern = re.compile(r'\\b[0-9a-fA-F]{64}\\b')\n",
    "    return {\n",
    "        'MD5_Hashes': re.findall(md5_pattern, content),\n",
    "        'SHA1_Hashes': re.findall(sha1_pattern, content),\n",
    "        'SHA256_Hashes': re.findall(sha256_pattern, content)\n",
    "    }\n",
    "\n",
    "def extract_registry_entries(content):\n",
    "    registry_pattern = re.compile(r'HK[A-Z_]+\\\\[^\"]+')\n",
    "    return re.findall(registry_pattern, content)\n",
    "\n",
    "def extract_urls(content):\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return re.findall(url_pattern, content)\n",
    "\n",
    "def categorize_content(content, rule_id, filename):\n",
    "    return {\n",
    "        'rule_id': rule_id,\n",
    "        'name': filename[:-4],\n",
    "        'description': \"-\",\n",
    "        'references': extract_references(content),\n",
    "        'File_Names': extract_filenames(content),\n",
    "        **extract_hashes(content),\n",
    "        'Registry_Entries': extract_registry_entries(content),\n",
    "        'URLs': extract_urls(content),\n",
    "        'weight': 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_files_from_directory(directory_path):\n",
    "    \"\"\"Returns a list of .txt files from the specified directory.\"\"\"\n",
    "    return [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.txt')]\n",
    "\n",
    "def process_text_file(file_path, unique_sets, rule_id):\n",
    "    \"\"\"Processes a single text file and returns the categorized content.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        filename = os.path.basename(file_path)\n",
    "        categorized_content = categorize_content(content, rule_id, filename)\n",
    "        categorized_content['references'] = list(set([x for x in categorized_content['references'] if x not in unique_sets['references']]))\n",
    "        unique_sets['references'].update(categorized_content['references'])\n",
    "        return categorized_content\n",
    "\n",
    "def write_to_json(output_path, content):\n",
    "    \"\"\"Writes the provided content to a JSON file.\"\"\"\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        json.dump(content, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist_urls = [\n",
    "    \"www.trendmicro.com\",\n",
    "    \"attack.mitre.org\",\n",
    "    \"documents.trendmicro.com\",\n",
    "    \"www.ithome.com.tw\",\n",
    "    \"hitcon.org\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file(file_path, unique_sets, rule_id):\n",
    "    \"\"\"Processes a single text file and returns the categorized content.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        filename = os.path.basename(file_path)\n",
    "        categorized_content = categorize_content(content, rule_id, filename)\n",
    "\n",
    "        # Ensure references are unique across all files\n",
    "        categorized_content['references'] = list(set([x for x in categorized_content['references'] if x not in unique_sets['references']]))\n",
    "        unique_sets['references'].update(categorized_content['references'])\n",
    "\n",
    "        # Ensure filenames are unique across all files\n",
    "        categorized_content['File_Names'] = list(set([x for x in categorized_content['File_Names'] if x not in unique_sets['File_Names']]))\n",
    "        unique_sets['File_Names'].update(categorized_content['File_Names'])\n",
    "\n",
    "        # Ensure hashes are unique across all files\n",
    "        for hash_type in ['MD5_Hashes', 'SHA1_Hashes', 'SHA256_Hashes']:\n",
    "            categorized_content[hash_type] = list(set([x for x in categorized_content[hash_type] if x not in unique_sets[hash_type]]))\n",
    "            unique_sets[hash_type].update(categorized_content[hash_type])\n",
    "\n",
    "        # Ensure registry entries are unique across all files\n",
    "        categorized_content['Registry_Entries'] = list(set([x for x in categorized_content['Registry_Entries'] if x not in unique_sets['Registry_Entries']]))\n",
    "        unique_sets['Registry_Entries'].update(categorized_content['Registry_Entries'])\n",
    "\n",
    "        # Ensure URLs are unique across all files\n",
    "        categorized_content['URLs'] = list(set([x for x in categorized_content['URLs'] if x not in unique_sets['URLs'] and not any(whitelisted in x for whitelisted in whitelist_urls)]))\n",
    "        unique_sets['URLs'].update(categorized_content['URLs'])\n",
    "\n",
    "        return categorized_content\n",
    "\n",
    "# Main processing function with adjustments for unique extraction\n",
    "\n",
    "def process_all_text_files(directory_path, output_path):\n",
    "    text_files = get_text_files_from_directory(directory_path)\n",
    "    unique_sets = {\n",
    "        'references': set(),\n",
    "        'File_Names': set(),\n",
    "        'MD5_Hashes': set(),\n",
    "        'SHA1_Hashes': set(),\n",
    "        'SHA256_Hashes': set(),\n",
    "        'Registry_Entries': set(),\n",
    "        'URLs': set()\n",
    "    }\n",
    "    rule_id = 0\n",
    "    for file_path in text_files:\n",
    "        rule_id += 1\n",
    "        categorized_content = process_text_file(file_path, unique_sets, rule_id)\n",
    "        filename_without_extension = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_json_path = os.path.join(output_path, filename_without_extension + \".json\")\n",
    "        write_to_json(output_json_path, categorized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_text_files(directory_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
